{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_bayesian import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Uniform, Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm2 = lambda x : (x**2).mean()\n",
    "neg_entropy = lambda p : p * torch.log(p) + (1-p) * torch.log(1-p)\n",
    "\n",
    "def get_layer(m,buffer,layer,output=\"list\"):\n",
    "    \"\"\"Function which takes a list and a model append the elements\"\"\"\n",
    "    for c in m.children():\n",
    "        if isinstance(c,layer):\n",
    "            if isinstance(buffer,list):\n",
    "              buffer.append(c)\n",
    "            elif isinstance(buffer,dict):\n",
    "              i = hex(id(c))\n",
    "              buffer[i] = c\n",
    "        get_layer(c,buffer,layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLU(nn.Module):\n",
    "  \"\"\"Probability Linear Unit\"\"\"\n",
    "  def __init__(self):\n",
    "    super(PLU,self).__init__()\n",
    "  \n",
    "  def forward(self,x):\n",
    "    z = torch.clamp(x,0,1)\n",
    "    return z    \n",
    "  \n",
    "class AutoDropout(nn.Module):\n",
    "    def __init__(self, dp=0., requires_grad=True):\n",
    "\n",
    "        super(AutoDropout, self).__init__()\n",
    "\n",
    "        # We transform the dropout rate to keep rate\n",
    "        p = 1 - dp\n",
    "        p = torch.tensor(p)\n",
    "\n",
    "        self.plu = PLU()\n",
    "\n",
    "        if requires_grad:\n",
    "            p = nn.Parameter(p)\n",
    "            self.register_parameter(\"p\", p)\n",
    "        else:\n",
    "            self.register_buffer(\"p\", p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, shape = x.shape[0], x.shape[1:]\n",
    "\n",
    "        # We make sure p is a probability\n",
    "        p = self.plu(self.p)\n",
    "\n",
    "        # We sample a mask\n",
    "        m = Bernoulli(p).sample(shape)\n",
    "\n",
    "        # Element wise multiplication\n",
    "        z = x * m\n",
    "\n",
    "        return z\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'p={}'.format(\n",
    "            self.p.item()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteDropout(nn.Module):\n",
    "    def __init__(self,t:float=0.1,dp:float=0.5,requires_grad=True):\n",
    "        super(ConcreteDropout,self).__init__()\n",
    "        \n",
    "        # We first invert the dropout rate to the keeping rate\n",
    "        p = 1 - dp\n",
    "        p = torch.tensor(p)\n",
    "        \n",
    "        if requires_grad:\n",
    "            p = nn.Parameter(p)\n",
    "            self.register_parameter(\"p\",p)\n",
    "        else:\n",
    "            self.register_buffer(\"p\",p)\n",
    "        \n",
    "        t = torch.tensor(t)\n",
    "        self.register_buffer(\"t\",t)\n",
    "        \n",
    "        self.u = Uniform(0,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        bs, shape = x.shape[0], x.shape[1:]\n",
    "        \n",
    "        u = self.u.sample(shape)\n",
    "        p = self.p.expand(shape)\n",
    "        \n",
    "        m = torch.sigmoid((torch.log(p) - torch.log(1-p) + torch.log(u) - torch.log(1-u)) / self.t)\n",
    "        \n",
    "        m = m[None]\n",
    "        \n",
    "        z = m * x\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dp_module,dp=0., bias=True, requires_grad=True):\n",
    "        super(DropLinear, self).__init__()\n",
    "\n",
    "        self.dp = dp_module(dp=dp,requires_grad=requires_grad)\n",
    "        self.W = nn.Linear(in_features=in_features,\n",
    "                           out_features=out_features, bias=bias)\n",
    "        self.W.weight.data = self.W.weight.data / self.W.weight.data.norm() * (1-dp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.W(x)\n",
    "        z = self.dp(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks.hooks import HookCallback\n",
    "\n",
    "class KLHook(HookCallback):\n",
    "    \"\"\"Hook to register the parameters of the latents during the forward pass to compute the KL term of the VAE\"\"\"\n",
    "    def __init__(self, learn,l:float=1e-2,do_remove:bool=True,recording=False):\n",
    "        super().__init__(learn)\n",
    "        \n",
    "        # First we store all the DropLinears layers to hook them \n",
    "        buffer = []\n",
    "        get_layer(learn.model,buffer,DropLinear)\n",
    "        if not buffer:\n",
    "            raise NotImplementedError(f\"No {DropLinear} Linear found\")\n",
    "            \n",
    "        self.modules = buffer\n",
    "        self.do_remove = do_remove\n",
    "        \n",
    "        # We will store the KL of each DropLinear here before summing them\n",
    "        self.kls = []\n",
    "        \n",
    "        self.N = len(learn.data.train_ds)\n",
    "        self.l = l\n",
    "        \n",
    "        self.recording = recording\n",
    "        \n",
    "        if recording:\n",
    "            self.stats = []\n",
    "            self.loss = []\n",
    "    \n",
    "    def on_backward_begin(self,last_loss,**kwargs):\n",
    "        \n",
    "        total_kl = 0\n",
    "        for kl in self.kls:\n",
    "            total_kl += kl\n",
    "            \n",
    "        total_kl /= self.N\n",
    "            \n",
    "        total_loss = last_loss + total_kl\n",
    "        \n",
    "        if self.recording:\n",
    "            self.loss.append({\"total_kl\":total_kl.item(),\"last_loss\":last_loss.item(),\n",
    "                            \"total_loss\":total_loss.item()})\n",
    "        \n",
    "        # We empty the buffer of kls\n",
    "        self.kls = []\n",
    "        \n",
    "        return {\"last_loss\" : total_loss}\n",
    "        \n",
    "    def hook(self, m:nn.Module, i, o):\n",
    "        \"Save the latents of the bottleneck\"\n",
    "        p = m.dp.p\n",
    "        \n",
    "        W = m.W.weight\n",
    "        norm_w = norm2(W)\n",
    "        \n",
    "        K_out = m.W.out_features\n",
    "        K_in = m.W.in_features\n",
    "        \n",
    "        l = (self.l ** 2) * K_in\n",
    "        \n",
    "        kl = l * p * norm_w / 2 + K_out * neg_entropy(p)       \n",
    "        \n",
    "        self.kls.append(kl)\n",
    "        \n",
    "        if self.recording:\n",
    "            i = hex(id(m))\n",
    "            self.stats.append({\"dropout\":1 - p.item(),\"w\":norm_w.item(),\"module\":i})\n",
    "    \n",
    "    def plot_stats(self):\n",
    "        assert self.recording, \"Recording mode was off during initialization\"\n",
    "        df = pd.DataFrame(self.stats)\n",
    "        df.plot()\n",
    "        \n",
    "    def plot_losses(self):\n",
    "        assert self.recording, \"Recording mode was off during initialization\"\n",
    "        df = pd.DataFrame(self.loss)\n",
    "        df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_module = ConcreteDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = DropLinear(4,4,dp_module,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.W.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImageDataBunch.from_folder(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12396"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.train_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
